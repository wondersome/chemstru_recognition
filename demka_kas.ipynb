{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-27T11:00:09.317990Z",
     "start_time": "2025-05-27T10:59:56.129142Z"
    }
   },
   "source": [
    "!pip install numpy==1.26.4  # Latest stable NumPy 1.x version\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from chemical_dataset import ChemicalDataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_images_from_pdf(pdf_path, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            print(f\"Processing {len(doc)} pages from {pdf_path}\")\n",
    "            all_images = []\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                img_list = page.get_images(full=True)\n",
    "                for img_index, img in enumerate(img_list):\n",
    "                    xref = img[0]\n",
    "                    base_image = doc.extract_image(xref)\n",
    "                    all_images.append({\n",
    "                        'page': page_num,\n",
    "                        'index': img_index,\n",
    "                        'image': base_image[\"image\"],\n",
    "                        'ext': base_image[\"ext\"]\n",
    "                    })\n",
    "                zoom = 2\n",
    "                mat = fitz.Matrix(zoom, zoom)\n",
    "                pix = page.get_pixmap(matrix=mat)\n",
    "                all_images.append({\n",
    "                    'page': page_num,\n",
    "                    'index': -1,\n",
    "                    'image': pix.tobytes(),\n",
    "                    'ext': 'png'\n",
    "                })\n",
    "            for img in all_images:\n",
    "                if img['index'] == -1:\n",
    "                    output_path = os.path.join(output_folder, f\"page_{img['page']+1}_full.png\")\n",
    "                else:\n",
    "                    output_path = os.path.join(output_folder, f\"page_{img['page']+1}_img_{img['index']}.{img['ext']}\")\n",
    "                with open(output_path, \"wb\") as f:\n",
    "                    f.write(img['image'])\n",
    "            return len(all_images)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "class ChemicalStructureSegmenter:\n",
    "    def __init__(self, min_structure_size=100, threshold=180, adaptive_thresh=True):\n",
    "        \"\"\"\n",
    "        Enhanced chemical structure detector with:\n",
    "        - Adaptive or fixed thresholding\n",
    "        - Contour filtering\n",
    "        - Structure validation\n",
    "        \"\"\"\n",
    "        self.min_size = min_structure_size\n",
    "        self.threshold = threshold\n",
    "        self.adaptive_thresh = adaptive_thresh\n",
    "\n",
    "    def segment_image(self, image_path, output_folder=None):\n",
    "        \"\"\"Improved segmentation with structure validation\"\"\"\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not read image: {image_path}\")\n",
    "        # Preprocessing pipeline\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.medianBlur(gray, 3)\n",
    "        gray = cv2.equalizeHist(gray)\n",
    "\n",
    "        # Thresholding\n",
    "        if self.adaptive_thresh:\n",
    "            thresh = cv2.adaptiveThreshold(\n",
    "                gray, 255,\n",
    "                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                cv2.THRESH_BINARY_INV, 11, 2\n",
    "            )\n",
    "        else:\n",
    "            _, thresh = cv2.threshold(\n",
    "                gray, self.threshold, 255,\n",
    "                cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU\n",
    "            )\n",
    "\n",
    "        # Morphological cleanup\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        structures = []\n",
    "        for i, cnt in enumerate(contours):\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            # Size filtering\n",
    "            if w < self.min_size or h < self.min_size:\n",
    "                continue\n",
    "            # Aspect ratio filtering\n",
    "            aspect_ratio = w / float(h)\n",
    "            if not (0.2 < aspect_ratio < 5.0):\n",
    "                continue\n",
    "            # Structure validation\n",
    "            roi = gray[y:y+h, x:x+w]\n",
    "            if self.is_chemical_structure(roi):\n",
    "                structure = image[y:y+h, x:x+w]\n",
    "                structures.append(structure)\n",
    "                if output_folder:\n",
    "                    os.makedirs(output_folder, exist_ok=True)\n",
    "                    output_path = os.path.join(output_folder, f\"structure_{i}.png\")\n",
    "                    cv2.imwrite(output_path, structure)\n",
    "        return structures\n",
    "\n",
    "    def is_chemical_structure(self, roi):\n",
    "        \"\"\"Validate if region contains a chemical structure\"\"\"\n",
    "        circles = cv2.HoughCircles(roi, cv2.HOUGH_GRADIENT, 1, 20,\n",
    "                                  param1=50, param2=30, minRadius=5, maxRadius=30)\n",
    "        edges = cv2.Canny(roi, 50, 150)\n",
    "        lines = cv2.HoughLinesP(edges, 1, np.pi/180, 50, minLineLength=20, maxLineGap=10)\n",
    "        return (circles is not None and len(circles) > 3) or (lines is not None and len(lines) > 5)\n",
    "\n",
    "def generate_chemical_images(output_dir=\"chemical_data\", num_samples=5000, augment=True):\n",
    "    classes = {\n",
    "        'alkane': ['CCCC', 'CCCCC', 'CC(C)C', 'CCCCCC'],\n",
    "        'alkene': ['C=CC', 'C=CCC', 'CC=CC', 'C=C(C)C'],\n",
    "        'alcohol': ['CCO', 'CCCO', 'CC(C)O', 'CCCCO'],\n",
    "        'carboxylic_acid': ['CC(=O)O', 'CCC(=O)O', 'CC(C)(=O)O'],\n",
    "        'amine': ['CN', 'CCN', 'CC(C)N', 'CCCN'],\n",
    "        'benzene': ['c1ccccc1', 'c1ccc(cc1)C', 'c1cc(ccc1)OC', 'c'],\n",
    "        'amide': ['CC(=O)N', 'CCC(=O)N', 'CC(C)(=O)N'],\n",
    "        'ether': ['COC', 'CCOC', 'CC(C)OC'],\n",
    "        'ketone': ['CC(=O)C', 'CCC(=O)C', 'CC(C)(=O)C'],\n",
    "        'aldehyde': ['CC=O', 'CCC=O', 'CC(C)=O'],\n",
    "        'ester': ['CCOC=O', 'CCCOC=O'],\n",
    "        'alkyne': ['C#C', 'C#CC', 'CC#CC'],\n",
    "        'nitrile': ['C#N', 'CC#N'],\n",
    "        'halide': ['CCl', 'CBr', 'CI'],\n",
    "        # ... other classes ...\n",
    "    }\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    samples_per_class = num_samples // len(classes)\n",
    "    remainder = num_samples % len(classes)\n",
    "    print(f\"Generating {num_samples} chemical structures across {len(classes)} classes...\")\n",
    "    for idx, (class_name, smiles_list) in enumerate(classes.items()):\n",
    "        class_dir = os.path.join(output_dir, class_name)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        current_samples = samples_per_class + (1 if idx < remainder else 0)\n",
    "        for i in tqdm(range(current_samples), desc=f\"Creating {class_name} images\"):\n",
    "            try:\n",
    "                smiles = np.random.choice(smiles_list)\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is None:\n",
    "                    continue\n",
    "                AllChem.Compute2DCoords(mol)\n",
    "                drawer = rdMolDraw2D.MolDraw2DCairo(400, 400)\n",
    "                options = drawer.drawOptions()\n",
    "                options.bondLineWidth = int(np.random.randint(1, 3))\n",
    "                options.highlightBondWidthMultiplier = 10 + np.random.randint(-5, 5)\n",
    "                options.atomLabelDeuteriumTritium = bool(np.random.choice([True, False]))\n",
    "                drawer.DrawMolecule(mol)\n",
    "                drawer.FinishDrawing()\n",
    "                img_bytes = drawer.GetDrawingText()\n",
    "                img = Image.open(BytesIO(img_bytes))\n",
    "                if augment:\n",
    "                    img = apply_image_augmentations(img)\n",
    "                output_path = os.path.join(class_dir, f\"{idx}_{i}.png\")\n",
    "                img.save(output_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating {class_name} {i}: {str(e)}\")\n",
    "                continue\n",
    "    print(f\"Synthetic dataset generated at '{output_dir}'\")\n",
    "    return list(classes.keys())\n",
    "\n",
    "def apply_image_augmentations(img):\n",
    "    img = np.array(img)\n",
    "    angle = np.random.uniform(-15, 15)\n",
    "    h, w = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "    img = cv2.warpAffine(img, M, (w, h), borderValue=(255, 255, 255))\n",
    "    scale = np.random.uniform(0.9, 1.1)\n",
    "    img = cv2.resize(img, None, fx=scale, fy=scale)\n",
    "    if np.random.rand() > 0.7:\n",
    "        noise = np.random.randint(0, 25, img.shape, dtype=np.uint8)\n",
    "        img = cv2.add(img, noise)\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "class ChemicalStructureRecognizer:\n",
    "    def __init__(self, num_classes, device=None, model_name='resnet50'):\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_name = model_name\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', patience=3)\n",
    "\n",
    "    def train(self, train_loader, val_loader=None, num_epochs=20):\n",
    "        \"\"\"Enhanced training loop with early stopping\"\"\"\n",
    "        best_acc = 0.0\n",
    "        best_model_wts = None\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "    \n",
    "            for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "            # Record metrics\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "    \n",
    "            val_loss, val_acc = 0.0, 0.0\n",
    "            if val_loader:\n",
    "                val_loss, val_acc = self.evaluate(val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                val_accuracies.append(val_acc)\n",
    "                self.scheduler.step(val_loss)\n",
    "    \n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)  # Use train loss as fallback\n",
    "                val_accuracies.append(0)\n",
    "    \n",
    "        if best_model_wts:\n",
    "            self.model.load_state_dict(best_model_wts)\n",
    "    \n",
    "        # ⚠️ THIS LINE WAS MISSING — ADD IT!\n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies\n",
    "        }\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        avg_loss = running_loss / len(data_loader)\n",
    "        accuracy = correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def predict(self, image, top_k=3):\n",
    "        self.model.eval()\n",
    "        if isinstance(image, str):\n",
    "            try:\n",
    "                image = Image.open(image).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image: {str(e)}\")\n",
    "                return None, None\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        image = transform(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            top_probs, top_classes = torch.topk(probs, top_k)\n",
    "        return top_classes.cpu().numpy()[0], top_probs.cpu().numpy()[0]\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        \n",
    "\n",
    "def run_full_pipeline(pdf_path, output_base_dir=\"chemical_output\",\n",
    "                     chembl_dir=\"chembl_35\", data_dir=\"chembl_images\",\n",
    "                     model_path=\"chemical_model.pth\"):\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "    extracted_dir = os.path.join(output_base_dir, \"extracted_pages\")\n",
    "    segmented_dir = os.path.join(output_base_dir, \"segmented_structures\")\n",
    "    results_dir = os.path.join(output_base_dir, \"results\")\n",
    "    recognized_dir = os.path.join(output_base_dir, \"recognized_structures\")\n",
    "    for dir_path in [extracted_dir, segmented_dir, results_dir, recognized_dir]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    print(\"\\nStep 1/5: Generating chemical structure images...\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        try:\n",
    "            class_mapping = generate_chemical_images(chembl_dir, data_dir, num_samples=50)\n",
    "            with open(os.path.join(results_dir, \"class_mapping.txt\"), \"w\") as f:\n",
    "                for key, value in class_mapping.items():\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating chemical images: {e}\")\n",
    "            print(\"Generating synthetic dataset instead...\")\n",
    "            class_mapping = generate_chemical_images(data_dir, num_samples=5000)\n",
    "    else:\n",
    "        print(\"Using existing chemical structure images\")\n",
    "    print(\"\\nStep 2/5: Preparing dataset...\")\n",
    "    try:\n",
    "        train_dataset = ChemicalDataset(data_dir, transform=train_transform, split='train')\n",
    "        val_dataset = ChemicalDataset(data_dir, transform=val_transform, split='val') \\\n",
    "            if os.path.exists(os.path.join(data_dir, 'val')) else None\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2) if val_dataset else None\n",
    "        print(f\"Found {len(train_dataset.classes)} classes\")\n",
    "        print(f\"Training samples: {len(train_dataset)}\")\n",
    "        if val_loader:\n",
    "            print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating datasets: {e}\")\n",
    "        print(\"Generating synthetic dataset and trying again...\")\n",
    "        class_mapping = generate_chemical_images(data_dir, num_samples=5000)\n",
    "        train_dataset = ChemicalDataset(data_dir, transform=train_transform, split='train')\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "        val_loader = None\n",
    "    print(\"\\nStep 3/5: Training model...\")\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    print(f\"Found {num_classes} classes: {train_dataset.classes}\")\n",
    "    model = ChemicalStructureRecognizer(num_classes=num_classes)\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing model from {model_path}\")\n",
    "        model.load(model_path)\n",
    "    else:\n",
    "        print(\"Training new model...\")\n",
    "        history = model.train(train_loader, val_loader, num_epochs=10)\n",
    "        model.save(model_path)\n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['train_losses'], label='Train Loss')\n",
    "        plt.plot(history['val_losses'], label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        val_accuracies = [acc for acc in history['val_accuracies'] if acc > 0]\n",
    "        if val_accuracies:\n",
    "            plt.plot(range(len(val_accuracies)), val_accuracies, label='Val Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Validation Accuracy')\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, \"training_history.png\"))\n",
    "        plt.close()\n",
    "    print(\"\\nExtracting content from PDF...\")\n",
    "    extracted_dir = os.path.join(output_base_dir, \"extracted_content\")\n",
    "    os.makedirs(extracted_dir, exist_ok=True)\n",
    "    extract_images_from_pdf(pdf_path, extracted_dir)\n",
    "    print(\"\\nProcessing extracted content...\")\n",
    "    chemical_structures_dir = os.path.join(output_base_dir, \"chemical_structures\")\n",
    "    os.makedirs(chemical_structures_dir, exist_ok=True)\n",
    "    segmenter = ChemicalStructureSegmenter(min_structure_size=100, threshold=160)\n",
    "    for item in os.listdir(extracted_dir):\n",
    "        if item.endswith(('.png','.jpg','.jpeg')):\n",
    "            img_path = os.path.join(extracted_dir, item)\n",
    "            structures = segmenter.segment_image(img_path, chemical_structures_dir)\n",
    "            print(f\"Found {len(structures)} structures in {item}\")\n",
    "    \n",
    "    print(\"\\nStep 6/6: Making predictions...\")\n",
    "    results = []\n",
    "    structure_count = 0\n",
    "    for root, dirs, files in os.walk(chemical_structures_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                structure_path = os.path.join(root, file)\n",
    "                pred_classes, probs = model.predict(structure_path)  # Get all top classes and probs\n",
    "                # Take the first (most confident) prediction\n",
    "                pred_classes, probs = model.predict(structure_path, top_k=num_classes)  # Get all classes\n",
    "                for pred_class, confidence in zip(pred_classes, probs):\n",
    "                    results.append({\n",
    "                        'structure_id': structure_count,\n",
    "                        'predicted_class': train_dataset.classes[pred_class],\n",
    "                        'confidence': float(confidence),\n",
    "                        'output_path': structure_path\n",
    "                    })\n",
    "                structure_count += 1\n",
    "                if structure_count % 10 == 0:\n",
    "                    print(f\"Processed {structure_count} structures...\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_csv = os.path.join(results_dir, \"recognition_results.csv\")\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\nProcessing complete! Results saved to {output_base_dir}\")\n",
    "    print(f\"Total structures recognized: {structure_count}\")\n",
    "    print(f\"Results CSV: {results_csv}\")\n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PDF_PATH = \"/Users/johnsnow/Downloads/chemstru_recognition/data/sample.pdf\"\n",
    "    results = run_full_pipeline(\n",
    "        pdf_path=PDF_PATH,\n",
    "        chembl_dir=\"/Users/johnsnow/Downloads/chemstru_recognition/chembl_35/chembl_35_sqlite\",\n",
    "        data_dir=\"chembl_images\",\n",
    "        model_path=\"chemical_model.pth\"\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /Users/johnsnow/Downloads/chemstru_recognition/.venv/lib/python3.12/site-packages (1.26.4)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\n",
      "Step 1/5: Generating chemical structure images...\n",
      "Using existing chemical structure images\n",
      "\n",
      "Step 2/5: Preparing dataset...\n",
      "Found 14 classes\n",
      "Training samples: 3721\n",
      "Validation samples: 940\n",
      "\n",
      "Step 3/5: Training model...\n",
      "Found 14 classes: ['alcohol', 'aldehyde', 'alkane', 'alkene', 'alkyne', 'amide', 'amine', 'benzene', 'carboxylic_acid', 'ester', 'ether', 'halide', 'ketone', 'nitrile']\n",
      "Loading existing model from chemical_model.pth\n",
      "\n",
      "Extracting content from PDF...\n",
      "Processing 1 pages from /Users/johnsnow/Downloads/chemstru_recognition/data/sample.pdf\n",
      "\n",
      "Processing extracted content...\n",
      "Found 8 structures in page_1_full.png\n",
      "Found 10 structures in page_1_img_0.jpeg\n",
      "\n",
      "Step 6/6: Making predictions...\n",
      "Processed 10 structures...\n",
      "Processed 20 structures...\n",
      "\n",
      "Processing complete! Results saved to chemical_output\n",
      "Total structures recognized: 21\n",
      "Results CSV: chemical_output/results/recognition_results.csv\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:54:18.289160Z",
     "start_time": "2025-05-27T08:54:18.283373Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b761ac3b88c57247",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
